[GNN]
no_cuda = False
ctd = 0
fastmode = False
seed = 123
n_features = 165
# 167??
n_classes = 2
#other normal
# gnns_forward_hidden = 256 256 256 256
# gnn_forward_layer_num = 4
# gnn_dropout = 0.5
#COMPARE without MLP / GCN GAT GRAPHSAGE
gnns_forward_hidden = 32 16 2
gnn_forward_layer_num = 3
gnn_dropout = 0.8
linears_hidden = 512 256
linear_layer_num = 2
bias = True
opt = Adam
adam_beta = 0.9 0.999
opt_momentum = 0.9
# previous 0。004
lr0 = 0.002
# previous 1
decay_rate = 0.6
weight_decay = 0.0001
gnn_do_bn = True
linear_do_bn = True
linear_dropout = 0.5
start_epoch = 0
epochs = 250

##########################  LS_GLAT part
#4. Single Graph + GAT + Long Term Layer Attention + Full Connection Layers (LSGLATModel)
# model_folder = LS_GLAT
# model_name = LS_GLAT
########################## 消融试验
#1. Single Graph + GAT + Long Term Layer Attention (GATNoFCModel)
# model_folder = GAT
# model_name = GATNoFCModel
#2. GAT + Full Connection Layers + Long Term Layer Attention (GATNoBidirectionalGraphModel)
# model_folder = GAT
# model_name = GATNoBidirectionalGraphModel
#3. Single Graph + GAT + Full Connection Layers (GATNoLTLAFCModel)
# model_folder = GAT
# model_name = GATNoLTLAFCModel
######################################3
# model_folder = DeeperGCN
# model_name = SINGLE_GRAPH_SAGE


[LTLA]
project_hidden = 128
tsf_dim = 256                        # Increased transformer dimension for better attention
tsf_mlp_hidden = 512                 # Enhanced MLP hidden layer size in LTLA
tsf_depth = 4                        # Retain optimal depth
tsf_heads = 8
tsf_head_dim = 16                    # Larger head dimension for improved representation
tsf_dropout = 0.5                    # Regularization in transformer layers
gt_emb_dropout = 0.5
gt_pool = mean