[GNN]
no_cuda = False
ctd = 0
fastmode = False
seed = 111
n_features = 165
# 167??
n_classes = 2
gnns_forward_hidden = 128 128 128 128
gnn_forward_layer_num = 4
linears_hidden = 258 128
linear_layer_num = 2
bias = True
opt = Adam
adam_beta = 0.9 0.999
opt_momentum = 0.9
# previous 0ã€‚004
lr0 = 0.001
# previous 1
decay_rate = 0.5
weight_decay = 0.0005
gnn_do_bn = True
linear_do_bn = True
gnn_dropout = 0.6
linear_dropout = 0.6
start_epoch = 0
epochs = 350

##########################  LS_GLAT part
# model_folder = LS_GLAT
# model_name = LS_GLAT
model_folder = DeeperGCN
model_name = SINGLE_GRAPH_SAGE

[LTLA]
project_hidden = 64
tsf_dim = 128
tsf_mlp_hidden = 256
tsf_depth = 4
# Number of attention heads for each attention layer in the Transformer encoder.
tsf_heads = 8
tsf_head_dim = 8
tsf_dropout = 0.6
gt_emb_dropout = 0.6
gt_pool = mean
